# Style Transfer with Diffusion and CNN Models

This project provides a unified implementation and comparison of state-of-the-art image style transfer algorithms, with a focus on **Stable Diffusion-based** and **CNN/Transformer-based** methods. The goal is to analyze, reproduce, and evaluate these methods for qualitative and quantitative comparison.

---

## ðŸ“ Project Structure

```
style-transfer/
â”‚
â”œâ”€â”€ Puff-Net/           # Transformer-based efficient style transfer
â”œâ”€â”€ StyleID/            # Diffusion-based training-free style transfer (cross-attention reweighting)
â”‚â”œâ”€â”€ content/            # Content images for style transfer
â”‚â”œâ”€â”€ style/              # Style images for reference
â”‚â””â”€â”€ eval/               # Evaluation scripts (ArtFID, LPIPS, CFSD, etc.)
â”œâ”€â”€ Zero-shot/          # Zâ˜…: Zero-shot style injection with attention manipulation
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ðŸ“š Included Methods and References

### ðŸ”¹ 1. **StyleID (Training-free Diffusion Style Transfer)**  
**Paper**: *Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer*  
**Authors**: Jiwoo Chung, Sangeek Hyun, Jae-Pil Heo  
**Conference**: CVPR 2024  
**Code Reference**: [https://github.com/jiwoogit/StyleID](https://github.com/jiwoogit/StyleID)

Key Features:
- Key & Value injection into self-attention layers
- Query preservation and attention temperature scaling
- Initial latent AdaIN for better color stylization

---

### ðŸ”¹ 2. **Zâ˜… (Zero-shot Style Injection for Diffusion Models)**  
**Paper**: *Zero-shot Style Transfer via Attention Reweighting*  
**Authors**: Z. Deng et al.  
**Conference**: CVPR 2024  
**Code Reference**: [https://github.com/HolmesShuan/Zero-shot-Style-Transfer-via-Attention-Rearrangement](https://github.com/HolmesShuan/Zero-shot-Style-Transfer-via-Attention-Rearrangement)
**Technique**:  
- Based on Stable Diffusion  
- Uses Reweighting of Cross-Attention Maps with Null-text Inversion  
- No training required, pure inference-based style injection

---

### ðŸ”¹ 3. **Puff-Net (Transformer with Pure Content & Style Feature Fusion)**  
**Paper**: *Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network*  
**Authors**: Sizhe Zheng, Pan Gao, Jie Qin  
**Conference**: CVPR 2024  
**Code Reference**: [https://github.com/ZszYmy9/Puff-Net](https://github.com/ZszYmy9/Puff-Net)

Highlights:
- Only uses Transformer encoder for low-latency inference
- Uses INN (Invertible Network) and LT blocks to disentangle content/style
- CAPE positional encoding and AdaIN-based loss

---

## ðŸ”§ Usage
> âœï¸ Each subfolder contains its own test/inference script.  
> Refer to individual README files or scripts inside each submodule.

---

## ðŸ“Š Evaluation

Metrics used:
- **FID**: FrÃ©chet Inception Distance
- **ArtFID**: Art-optimized FID
- **LPIPS**: Learned Perceptual Image Patch Similarity
- **CFSD**: Content Feature Structural Distance

Evaluation scripts are located in `eval/`.

---

## ðŸ“„ License

This project contains original implementations and adapted code from official repositories. See individual submodules for license details.
